{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# OpenAI - Gym 指南"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 初始化环境\n",
    "首先需要安装 gym package，可以通过 pip 或者 conda 等完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "调用gym.make()函数，传入一个现存的环境名，可以返回一个该环境的实例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "\n",
    "from gym import spaces\n",
    "from gym.spaces import Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def make_multi_agent(env_name_or_creator):\n",
    "    \"\"\"Convenience wrapper for any single-agent env to be converted into MA.\n",
    "    Agent IDs are int numbers starting from 0 (first agent).\n",
    "    Args:\n",
    "        env_name_or_creator (Union[str, Callable[]]: String specifier or\n",
    "            env_maker function.\n",
    "    Returns:\n",
    "        Type[MultiAgentEnv]: New MultiAgentEnv class to be used as env.\n",
    "            The constructor takes a config dict with `num_agents` key\n",
    "            (default=1). The reset of the config dict will be passed on to the\n",
    "            underlying single-agent env's constructor.\n",
    "    Examples:\n",
    "         # >>> # By gym string:\n",
    "         # >>> ma_cartpole_cls = make_multi_agent(\"CartPole-v0\")\n",
    "         # >>> # Create a 2 agent multi-agent cartpole.\n",
    "         # >>> ma_cartpole = ma_cartpole_cls({\"num_agents\": 2})\n",
    "         # >>> obs = ma_cartpole.reset()\n",
    "         # >>> print(obs)\n",
    "         # ... {0: [...], 1: [...]}\n",
    "         # >>> # By env-maker callable:\n",
    "         # >>> ma_stateless_cartpole_cls = make_multi_agent(\n",
    "         # ...    lambda config: StatelessCartPole(config))\n",
    "         # >>> # Create a 2 agent multi-agent stateless cartpole.\n",
    "         # >>> ma_stateless_cartpole = ma_stateless_cartpole_cls(\n",
    "         # ...    {\"num_agents\": 2})\n",
    "    \"\"\"\n",
    "\n",
    "    class MultiEnv(MultiAgentEnv):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            self.env = env_name_or_creator(config)\n",
    "            obs = self.env.observation_space\n",
    "            self.observation_space = {i: spaces.Tuple((obs[0], Box(obs[1].low[0], obs[1].high[0], shape=(1,),\n",
    "                                                                   dtype=np.int64), obs[2])) for i in range(self.env.number_agents)}\n",
    "\n",
    "            self.action_space = self.env.action_space\n",
    "\n",
    "        def reset(self):\n",
    "            obs_state = self.env.reset()\n",
    "            agents_state = obs_state[1]\n",
    "            return {i: (obs_state[0], np.array([agents_state[i]]), np.array(obs_state[2])) for i in range(self.env.number_agents)}\n",
    "\n",
    "        def step(self, action_dict):\n",
    "            obs, rew, dones, info = {}, {}, {}, {}\n",
    "            action_list = []\n",
    "            for i, action in action_dict.items():\n",
    "                action_list.append(action)\n",
    "            joint_obs_np, joint_reward_np, done, _ = self.env.step(action_list)\n",
    "            agents_state = joint_obs_np[1]\n",
    "            dones[\"__all__\"] = done\n",
    "            rew = {i: joint_reward_np for i in range(self.env.number_agents)}\n",
    "            obs = {i: (joint_obs_np[0], np.array([agents_state[i]]), np.array(joint_obs_np[2])) for i in range(self.env.number_agents)}\n",
    "            return obs, rew, dones, info\n",
    "\n",
    "    return MultiEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "obs = env.observation_space\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env.reset()\n",
    "env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    if done == \n",
    "    env.step(0)\n",
    "    print(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## MountainCar example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. **首先查看观测空间和动作空间**  \n",
    "Box 代表连续空间，观测空间为浮点型的二维np.array。  \n",
    "Discrete 代表离散空间，动作空间为（0.1.2）的整型数值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "观测空间 = Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n",
      "动作空间 = Discrete(3)\n",
      "观测范围 = [-1.2  -0.07] ~ [0.6  0.07]\n",
      "动作数 = 3\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('MountainCar-v0')\n",
    "print('观测空间 = {}'.format(env.observation_space))\n",
    "print('动作空间 = {}'.format(env.action_space))\n",
    "print('观测范围 = {} ~ {}'.format(env.observation_space.low,\n",
    "        env.observation_space.high))\n",
    "print('动作数 = {}'.format(env.action_space.n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2. **接下来实现一个智能体类 - BespokeAgent类**  \n",
    "智能体往往都是我们自己实现的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class BespokeAgent:\n",
    "    def __init__(self, env):\n",
    "        pass\n",
    "    \n",
    "    def decide(self, observation): # 决策\n",
    "        position, velocity = observation\n",
    "        lb = min(-0.09 * (position + 0.25) ** 2 + 0.03,\n",
    "                0.3 * (position + 0.9) ** 4 - 0.008)\n",
    "        ub = -0.07 * (position + 0.38) ** 2 + 0.07\n",
    "        if lb < velocity < ub:\n",
    "            action = 2\n",
    "        else:\n",
    "            action = 0\n",
    "        return action # 返回动作\n",
    "\n",
    "    def learn(self, *args): # 学习\n",
    "        pass\n",
    "    \n",
    "agent = BespokeAgent(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def play_montecarlo(env, agent, render=False, train=False):\n",
    "    episode_reward = 0. # 记录回合总奖励，初始化为0\n",
    "    observation = env.reset() # 重置游戏环境，开始新回合\n",
    "    while True: # 不断循环，直到回合结束\n",
    "        if render: # 判断是否显示\n",
    "            env.render() # 显示图形界面，图形界面可以用 env.close() 语句关闭\n",
    "        action = agent.decide(observation)\n",
    "        next_observation, reward, done, _ = env.step(action) # 执行动作\n",
    "        episode_reward += reward # 收集回合奖励\n",
    "        if train: # 判断是否训练智能体\n",
    "            agent.learn(observation, action, reward, done) # 学习\n",
    "        if done: # 回合结束，跳出循环\n",
    "            break\n",
    "        observation = next_observation\n",
    "    return episode_reward # 返回回合总奖励\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%pip install pyglet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "回合奖励 = -105.0\n"
     ]
    }
   ],
   "source": [
    "env.seed(0) # 设置随机数种子,只是为了让结果可以精确复现,一般情况下可删去\n",
    "episode_reward = play_montecarlo(env, agent, render=True)\n",
    "print('回合奖励 = {}'.format(episode_reward))\n",
    "env.close() # 此语句可关闭图形界面\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%pip install numpy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "episode_rewards = [play_montecarlo(env, agent) for _ in range(100)]\n",
    "print('平均回合奖励 = {}'.format(np.mean(episode_rewards)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "90ae384ec8aa6644af31da6e68cfa043ca5e4f959255e106df86226e1926c12a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}